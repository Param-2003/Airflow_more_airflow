# ğŸ› ï¸ Apache Airflow Projects Portfolio

This repository showcases my hands-on learning and practice with Apache Airflow, focusing on real-world data engineering use cases. Each project is containerized using Docker and scheduled with Airflow DAGs.

---

## ğŸ§° Tech Stack
- Apache Airflow
- Python
- Docker
- Pandas / PySpark (for ETL)
- Jinja templating
- Kafka (for advanced pipeline)
- AWS S3 (optional, simulated locally)

---

## ğŸ“‚ Project Structure

| Folder | Description |
|--------|-------------|
| `dags/` | All DAG definitions â€” XCom, templating, branching, ETL jobs |
| `etl_scripts/` | ETL logic (data cleaning, transformation) |
| `data/` | Sample datasets for input/output |
| `notebooks/` | Jupyter notebooks to test scripts |
| `docker-compose.yaml` | Spins up Airflow locally using Docker |

---

## ğŸ“Œ DAGs Included

- `xcom_example.py` â€“ Sharing data between tasks using XCom
- `branching_example.py` â€“ Conditional task branching
- `templating_example.py` â€“ Jinja templating in BashOperator
- `micro_batch_etl.py` â€“ Simulated micro-batching with data polling
- `streaming_trigger_dag.py` â€“ Orchestrating a streaming-like job

---

## ğŸš€ Setup Instructions

1. Clone the repo:
   ```bash
   git clone https://github.com/<your-username>/airflow-projects-portfolio.git
   cd airflow-projects-portfolio
